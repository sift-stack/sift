"""
Low-level wrapper for the IngestionAPI.

This module provides thin wrappers around the autogenerated bindings for the IngestionAPI.
It handles common concerns like error handling and retries.

It provides an asynchronous client for the IngestionAPI.
"""

from __future__ import annotations

import asyncio
import atexit
import hashlib
import logging
import sys
import threading
from collections import namedtuple
from datetime import datetime
from queue import Queue
from typing import Any, Dict, List, cast

import sift_stream_bindings
from sift.ingest.v1.ingest_pb2 import (
    IngestArbitraryProtobufDataStreamResponse,
)
from sift.ingestion_configs.v2.ingestion_configs_pb2 import (
    GetIngestionConfigRequest,
    ListIngestionConfigFlowsResponse,
    ListIngestionConfigsRequest,
    ListIngestionConfigsResponse,
)
from sift.ingestion_configs.v2.ingestion_configs_pb2_grpc import IngestionConfigServiceStub
from sift_stream_bindings import (
    ChannelBitFieldElementPy,
    ChannelConfigPy,
    ChannelEnumTypePy,
    FlowConfigPy,
    IngestionConfigFormPy,
    IngestWithConfigDataStreamRequestPy,
)

from sift_client._internal.low_level_wrappers.base import (
    LowLevelClientBase,
)
from sift_client.transport import GrpcClient, WithGrpcClient
from sift_client.types.channel import Flow
from sift_client.types.ingestion import IngestionConfig
from sift_client.util import cel_utils as cel
from sift_client.util.timestamp import to_rust_py_timestamp

logger = logging.getLogger(__name__)

logger.setLevel(logging.DEBUG)

handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.DEBUG)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


class IngestionThread(threading.Thread):
    """
    Manages ingestion for a single ingestion config.
    """

    def __init__(self, sift_stream: sift_stream_bindings.SiftStreamPy, data_queue: Queue):
        """
        Initialize the IngestionThread.

        Args:
            sift_stream: The sift stream to use for ingestion.
            data_queue: The queue to put IngestWithConfigDataStreamRequestPy requests into for ingestion.
        """
        super().__init__(daemon=True)
        self.sift_stream = sift_stream
        self.data_queue = data_queue
        self._stop = threading.Event()

    def stop(self):
        self._stop.set()

    async def _run(self):
        backup_queue = []
        logger.debug("Ingestion thread started")

        while True:
            try:
                # Process all items in the queue
                count = 0
                while not self.data_queue.empty():
                    item = self.data_queue.get()
                    # None signals the main thread is done
                    if item is None:
                        self._stop.set()
                        logger.info("Ingestion thread finishing.")
                        await self.sift_stream.finish()
                        return
                    # Store each item in a backup queue in case we lose the connection
                    backup_queue.append(item)
                    result = await self.sift_stream.send_requests(item)
                    self.sift_stream = result
                    count += 1
                    if count % 100 == 0:
                        logger.debug(
                            f"Ingestion thread sent {count} requests, remaining: {self.data_queue.qsize()}"
                        )

                # If we get here, the queue is empty
                if self._stop.is_set():
                    return

            except Exception as e:
                logger.warning(f"{type(e)}: Error in ingestion thread: {e}")
                print(f"{type(e)}: Error in ingestion thread: {e}", flush=True)
                # Add the data back to the queue so that we can stream it again
                for item in backup_queue:
                    self.data_queue.put(item)
                backup_queue.clear()

    def run(self):
        """This thread will handle sending data to Sift."""
        # Even thought this is a thread, we need to run this async task to await send_requests otherwise we get sift_stream consumed errors.
        asyncio.run(self._run())


class IngestionLowLevelClient(LowLevelClientBase, WithGrpcClient):
    """
    Low-level client for the IngestionAPI.

    This class provides a thin wrapper around the autogenerated bindings for the IngestionAPI.
    It handles common concerns like error handling and retries.
    """

    CacheEntry = namedtuple("CacheEntry", ["sift_stream", "data_queue", "thread"])

    sift_stream_builder: sift_stream_bindings.SiftStreamBuilderPy
    stream_cache: Dict[str, "CacheEntry"] = {}

    def __init__(self, grpc_client: GrpcClient):
        """
        Initialize the IngestionLowLevelClient.

        Args:
            grpc_client: The gRPC client to use for making API calls.
        """
        super().__init__(grpc_client=grpc_client)
        # Rust GRPC client expects URI to have http(s):// prefix.
        uri = grpc_client._config.uri
        if not uri.startswith("http"):
            uri = f"https://{uri}" if grpc_client._config.use_ssl else f"http://{uri}"
        self.sift_stream_builder = sift_stream_bindings.SiftStreamBuilderPy(
            uri=uri,
            apikey=grpc_client._config.api_key,
        )
        self.sift_stream_builder.enable_tls = grpc_client._config.use_ssl
        self.sift_stream_builder.recovery_strategy = (
            sift_stream_bindings.RecoveryStrategyPy.retry_with_in_memory_backups(
                sift_stream_bindings.RetryPolicyPy(
                    max_attempts=5,
                    initial_backoff=sift_stream_bindings.DurationPy(secs=0, nanos=50_000_000),
                    max_backoff=sift_stream_bindings.DurationPy(secs=5, nanos=0),
                    backoff_multiplier=5,
                ),
                max_buffer_size=None,
            )
        )

        atexit.register(self.cleanup, timeout=0.1)

    def cleanup(self, timeout: float | None = None):
        """
        Cleanup the ingestion threads.

        Args:
            timeout: The timeout in seconds to wait for ingestion to complete. If None, will wait forever.
        """
        for _, cache_entry in self.stream_cache.items():
            sift_stream, data_queue, thread = cache_entry
            # "None" value on the queue signals its loop to terminate.
            data_queue.put(None)
            # Block for timeout to ensure the thread has time to finish and force it to stop if not.
            thread.join(timeout=timeout)
            if thread.is_alive():
                logger.error(
                    f"Ingestion thread did not finish after {timeout} seconds. Forcing stop."
                )
                thread.stop()

    async def get_ingestion_config_flows(self, ingestion_config_id: str) -> List[Flow]:
        """
        Get the flows for an ingestion config.
        """
        res = await self._grpc_client.get_stub(IngestionConfigServiceStub).GetIngestionConfig(
            GetIngestionConfigRequest(ingestion_config_id=ingestion_config_id)
        )
        res = cast(ListIngestionConfigFlowsResponse, res)
        return [Flow._from_proto(flow) for flow in res.flows]

    async def list_ingestion_configs(self, filter_query: str) -> List[IngestionConfig]:
        """
        List ingestion configs.
        """
        res = await self._grpc_client.get_stub(IngestionConfigServiceStub).ListIngestionConfigs(
            ListIngestionConfigsRequest(filter=filter_query)
        )
        res = cast(ListIngestionConfigsResponse, res)
        return [IngestionConfig._from_proto(config) for config in res.ingestion_configs]

    async def get_ingestion_config_id_from_client_key(self, client_key: str) -> str | None:
        """
        Get the ingestion config id.
        """
        filter_query = cel.equals("client_key", client_key)
        ingestion_configs = await self.list_ingestion_configs(filter_query)
        if not ingestion_configs:
            return None
        if len(ingestion_configs) > 1:
            raise ValueError(
                f"Expected 1 ingestion config for client key {client_key}, got {len(ingestion_configs)}"
            )
        return ingestion_configs[0].id

    def _new_ingestion_thread(
        self, ingestion_config_id: str, sift_stream: sift_stream_bindings.SiftStreamPy
    ):
        """Start a new ingestion thread.
        This allows ingestion to happen in the background regardless of if the user is using the sync or async client
        and without them having to set up threading themselves. We are using a thread vs asyncio since our
        sync wrapper will block on incomlete tasks.

        Args:
            ingestion_config_id: The id of the ingestion config for the flows this stream will ingest. Used to cache the stream.
            sift_stream: The sift stream to use for ingestion.
        """
        data_queue: Queue[List[IngestWithConfigDataStreamRequestPy]] = Queue()
        if ingestion_config_id in self.stream_cache:
            raise ValueError(
                f"Ingestion config {ingestion_config_id} already exists. This should not happen."
            )
        thread = IngestionThread(sift_stream, data_queue)
        thread.start()
        self.stream_cache[ingestion_config_id] = (sift_stream, data_queue, thread)  # type: ignore

    def _hash_flows(self, asset_name: str, flows: List[Flow]) -> str:
        """
        Generate a client key that should be unique but deterministic for the given asset and flow configuration.
        """
        # TODO:  Taken from sift_py/ingestion/config/telemetry.py. Confirm intent from Marc.
        m = hashlib.sha256()
        m.update(asset_name.encode())
        for flow in sorted(flows, key=lambda f: f.name):
            m.update(flow.name.encode())
            # Do not sort channels in alphabetical order since order matters.
            for channel in flow.channels:
                m.update(channel.name.encode())
                # Use api_format for data type since that should be consistent between languages.
                m.update(channel.data_type.as_human_str(api_format=True).encode())
                m.update((channel.description or "").encode())
                # Deprecated.
                m.update((channel.component or "").encode())
                m.update((channel.unit or "").encode())
                if channel.bit_field_elements:
                    for bfe in sorted(channel.bit_field_elements, key=lambda bfe: bfe.index):
                        m.update(bfe.name.encode())
                        m.update(str(bfe.index).encode())
                        m.update(str(bfe.bit_count).encode())
                if channel.enum_types:
                    for enum_name, enum_key in sorted(
                        channel.enum_types.items(), key=lambda it: it[1]
                    ):
                        m.update(str(enum_key).encode())
                        m.update(enum_name.encode())

        return m.hexdigest()

    async def create_ingestion_config(
        self,
        *,
        asset_name: str,
        flows: List[Flow],
        client_key: str | None = None,
        organization_id: str | None = None,
    ) -> str:
        """
        Create an ingestion config.

        Args:
            asset_name: The name of the asset to ingest to.
            flows: The flows to ingest.
            client_key: The client key to use for ingestion. If not provided, a new one will be generated.
            organization_id: The organization id to use for ingestion. Only needed if the user is part of several organizations.

        Returns:
            The id of the new or found ingestion config.
        """
        ingestion_config_id = None
        if client_key:
            print(f"Getting ingestion config id for client key {client_key}")
            ingestion_config_id = await self.get_ingestion_config_id_from_client_key(client_key)
            print(f"Getting ingestion config flows for ingestion config id {ingestion_config_id}")
            # Perform validation that the flows are valid for the ingestion config.
            existing_flows = await self.get_ingestion_config_flows(ingestion_config_id)
            for flow in flows:
                if flow.name in {existing_flow.name for existing_flow in existing_flows}:
                    raise ValueError(
                        f"Flow {flow.name} already exists for ingestion client {client_key}"
                    )
        else:
            client_key = self._hash_flows(asset_name, flows)
            try:
                logger.debug(f"Getting ingestion config id from generated client key {client_key}")
                ingestion_config_id = await self.get_ingestion_config_id_from_client_key(client_key)
            except ValueError:
                logging.debug(
                    f"No ingestion config found for client key {client_key}. Creating new one."
                )
                pass

        sift_stream, data_queue, ingestion_task = (
            self.stream_cache.get(ingestion_config_id, (None, None, None))
            if ingestion_config_id
            else (None, None, None)
        )
        if sift_stream is None:
            if ingestion_config_id:
                # This happens nominally if the ingestion config was created previously (i.e. specific client key or starting new run w/ same flow definitions).
                logger.debug(
                    f"Ingestion config {ingestion_config_id} already exists but is not yet in the cache."
                )
            # TODO: make converter functions
            flow_configs = []
            for flow in flows:
                flow_configs.append(
                    FlowConfigPy(
                        name=flow.name,
                        channels=[
                            ChannelConfigPy(
                                name=channel.name,
                                data_type=channel.data_type.to_rust_type(),
                                description=channel.description or "",
                                unit=channel.unit or "",
                                bit_field_elements=[
                                    ChannelBitFieldElementPy(
                                        name=bfe.name, index=bfe.index, bit_count=bfe.bit_count
                                    )
                                    for bfe in channel.bit_field_elements or []
                                ],
                                enum_types=[
                                    ChannelEnumTypePy(key=enum_key, name=enum_name)
                                    for enum_name, enum_key in channel.enum_types.items()
                                ]
                                if channel.enum_types
                                else [],
                            )
                            for channel in flow.channels
                        ],
                    )
                )
            ingestion_config = IngestionConfigFormPy(
                asset_name=asset_name,
                flows=flow_configs,
                client_key=client_key,
            )
            self.sift_stream_builder.ingestion_config = ingestion_config
            print(
                f"Builder: {self.sift_stream_builder.apikey}, {self.sift_stream_builder.uri}, {self.sift_stream_builder.enable_tls}"
            )
            sift_stream = await self.sift_stream_builder.build()

            ingestion_config_id = await self.get_ingestion_config_id_from_client_key(client_key)
            assert ingestion_config_id is not None, (
                "No ingestion config id found after building new stream. Likely server error."
            )
            print(f"Built new stream for ingestion config {ingestion_config_id}")
            self._new_ingestion_thread(ingestion_config_id, sift_stream)

        for flow in flows:
            flow.ingestion_config_id = ingestion_config_id

        if not ingestion_config_id:
            raise ValueError("No ingestion config id found")
        return ingestion_config_id

    def wait_for_ingestion_to_complete(self, timeout: float | None = None):
        """
        Blocks until all ingestion to complete.

        Args:
            timeout: The timeout in seconds to wait for ingestion to complete. If None, will wait forever.
        """
        self.cleanup(timeout)

    def ingest_flow(
        self,
        *,
        flow: Flow,
        timestamp: datetime,
        channel_values: dict[str, Any],
        organization_id: str | None = None,
    ):
        """
        Ingest a flow. This is a synchronous call that queues an ingestion request that will be processed asynchronously on a background thread.

        Args:
            flow: The flow to ingest.
            timestamp: The timestamp of the flow.
            channel_values: The channel values to ingest.
            organization_id: The organization id to use for ingestion. Only relevant if the user is part of several organizations.
        """

        if not flow.ingestion_config_id:
            raise ValueError(
                "Flow has no ingestion config id -- have you created an ingestion config for this flow?"
            )
        sift_stream, data_queue, ingestion_task = self.stream_cache.get(
            flow.ingestion_config_id, (None, None, None)
        )
        if sift_stream is None:
            raise ValueError(
                f"Ingestion config {flow.ingestion_config_id} not found. Have you created an ingestion config for this flow?"
            )
        rust_channel_values = []
        # Iterate through all expected channels for flow and convert to ingestion types (missing channels use a special empty type)
        for channel in flow.channels:
            val = channel_values.get(channel.name)
            rust_channel_values.append(channel.to_rust_value(val))
        req = IngestWithConfigDataStreamRequestPy(
            ingestion_config_id=flow.ingestion_config_id,
            run_id=flow.run_id,
            flow=flow.name,
            timestamp=to_rust_py_timestamp(timestamp),
            channel_values=rust_channel_values,
            end_stream_on_validation_error=False,
            organization_id=organization_id or "",  # This will be filled in by the server
        )
        assert data_queue is not None
        data_queue.put([req])

    async def ingest_arbitrary_protobuf_data_stream(
        self,
    ) -> IngestArbitraryProtobufDataStreamResponse:
        """
        Stream arbitrary protobuf data for ingestion.

        Returns:
            The ingestion response.
        """
        raise NotImplementedError("Not implemented")
