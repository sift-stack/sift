"""
Low-level wrapper for the IngestionAPI.

This module provides thin wrappers around the autogenerated bindings for the IngestionAPI.
It handles common concerns like error handling and retries.

It provides an asynchronous client for the IngestionAPI.
"""

from __future__ import annotations

import asyncio
import hashlib
import logging
import threading
from datetime import datetime
from queue import Queue
from typing import Any, List, cast

import sift_stream_bindings
from sift.ingest.v1.ingest_pb2 import (
    IngestArbitraryProtobufDataStreamResponse,
)
from sift.ingestion_configs.v2.ingestion_configs_pb2 import (
    GetIngestionConfigRequest,
    ListIngestionConfigFlowsResponse,
    ListIngestionConfigsRequest,
    ListIngestionConfigsResponse,
)
from sift.ingestion_configs.v2.ingestion_configs_pb2 import IngestionConfig as IngestionConfigProto
from sift.ingestion_configs.v2.ingestion_configs_pb2_grpc import IngestionConfigServiceStub
from sift_stream_bindings import (
    ChannelBitFieldElementPy,
    ChannelConfigPy,
    ChannelEnumTypePy,
    FlowConfigPy,
    IngestionConfigFormPy,
    IngestWithConfigDataStreamRequestPy,
    TimeValuePy,
)

from sift_client._internal.low_level_wrappers.base import (
    LowLevelClientBase,
)
from sift_client.transport import GrpcClient, WithGrpcClient
from sift_client.types.channel import ChannelDataType, Flow
from sift_client.util import cel_utils as cel

logger = logging.getLogger(__name__)


class IngestionLowLevelClient(LowLevelClientBase, WithGrpcClient):
    """
    Low-level client for the IngestionAPI.

    This class provides a thin wrapper around the autogenerated bindings for the IngestionAPI.
    It handles common concerns like error handling and retries.
    """

    sift_stream_builder: sift_stream_bindings.SiftStreamBuilderPy
    sift_stream: sift_stream_bindings.SiftStreamPy

    def __init__(self, grpc_client: GrpcClient):
        """
        Initialize the IngestionLowLevelClient.

        Args:
            grpc_client: The gRPC client to use for making API calls.
        """
        super().__init__(grpc_client=grpc_client)
        self.sift_stream_builder = sift_stream_bindings.SiftStreamBuilderPy(
            uri=grpc_client._config.uri,
            apikey=grpc_client._config.api_key,
        )
        self.sift_stream_builder.enable_tls = grpc_client._config.use_ssl

    async def get_ingestion_config_flows(self, ingestion_config_id: str) -> List[Flow]:
        """
        Get the flows for an ingestion config.
        """
        res = await self._grpc_client.get_stub(IngestionConfigServiceStub).GetIngestionConfig(
            GetIngestionConfigRequest(ingestion_config_id=ingestion_config_id)
        )
        res = cast(ListIngestionConfigFlowsResponse, res)
        return [Flow._from_proto(flow, self._grpc_client) for flow in res.flows]

    # TODO: Change to not return proto objects.
    async def list_ingestion_configs(self, filter_query: str) -> List[IngestionConfigProto]:
        """
        List ingestion configs.
        """
        res = await self._grpc_client.get_stub(IngestionConfigServiceStub).ListIngestionConfigs(
            ListIngestionConfigsRequest(filter_query=filter_query)
        )
        res = cast(ListIngestionConfigsResponse, res)
        return res.ingestion_configs

    async def get_ingestion_config_id_from_client_key(self, client_key: str) -> str:
        """
        Get the ingestion config id.
        """
        filter_query = cel.equals("client_key", client_key)
        ingestion_configs = await self.list_ingestion_configs(filter_query)
        if len(ingestion_configs) != 1:
            raise ValueError(
                f"Expected 1 ingestion config for client key {client_key}, got {len(ingestion_configs)}"
            )
        return ingestion_configs[0].ingestion_config_id

    async def _ingestion_thread(self, data_queue: Queue):
        """This thread will handle sending data to Sift."""
        stop = threading.Event()
        backup_queue = []
        if not self.sift_stream:
            raise ValueError("Sift stream not initialized")

        while True:
            try:
                # Process all items in the queue
                while not data_queue.empty():
                    item = data_queue.get()
                    # None signals the main thread is done
                    if item is None:
                        stop.set()
                        logger.info("Main thread completed.")
                        await self.sift_stream.finish()
                        return
                    # Store each item in a backup queue in case we lose the connection
                    backup_queue.append(item)
                    result = await self.sift_stream.send_requests(item)
                    self.sift_stream = result

                # If we get here, the queue is empty
                if stop.is_set():
                    return

            except Exception as e:
                logger.warning(f"Error in ingestion thread: {e}")
                # Add the data back to the queue so that we can stream it again
                for item in backup_queue:
                    data_queue.put(item)
                backup_queue.clear()

    async def create_ingestion_config(
        self,
        *,
        asset_name: str,
        flows: List[Flow],
        client_key: str | None = None,
        organization_id: str | None = None,
    ) -> str:
        """
        Create an ingestion config.
        """
        if client_key:
            # TODO: Add note about making client key management more ergonomic.
            # TODO: Cache
            existing_flows = await self.get_ingestion_config_flows(client_key)
            for flow in flows:
                if flow.name in {existing_flow.name for existing_flow in existing_flows}:
                    raise ValueError(
                        f"Flow {flow.name} already exists for ingestion client {client_key}"
                    )
        else:
            client_key = self.hash(asset_name, flows)
        # req = CreateIngestionConfigRequest(
        #     asset_name=asset_name,
        #     flows=[flow.to_proto() for flow in flows],
        #     organization_id=organization_id or "",
        #     client_key=client_key,
        # )
        # res = await self._grpc_client.get_stub(IngestionConfigServiceStub).CreateIngestionConfig(
        #     req
        # )
        # res = cast(CreateIngestionConfigResponse, res)
        # ingestion_config_id = res.ingestion_config.ingestion_config_id
        # for flow in flows:
        #     flow.ingestion_config_id = ingestion_config_id

        # TODO: make converter functions
        ingestion_config = IngestionConfigFormPy(
            asset_name=asset_name,
            flows=[
                FlowConfigPy(
                    name=flow.name,
                    channels=[
                        ChannelConfigPy(
                            name=channel.name,
                            data_type=channel.data_type.as_human_str(api_format=True),
                            description=channel.description,
                            component=channel.component,
                            unit=channel.unit,
                            bit_field_elements=[
                                ChannelBitFieldElementPy(
                                    name=bfe.name, index=bfe.index, bit_count=bfe.bit_count
                                )
                                for bfe in channel.bit_field_elements
                            ],
                            enum_types=[
                                ChannelEnumTypePy(key=enum.key, name=enum.name)
                                for enum in channel.enum_types
                            ],
                        )
                        for channel in flow.channels
                    ],
                )
                for flow in flows
            ],
            client_key=client_key,
        )
        if self.sift_stream_builder.ingest_config:
            raise ValueError("Ingestion config already set. Not sure how to handle this yet.")
        self.sift_stream_builder.ingest_config = ingestion_config
        self.sift_stream = self.sift_stream_builder.build()
        ingestion_config_id = self.get_ingestion_config_id_from_client_key(client_key)
        for flow in flows:
            flow.ingestion_config_id = ingestion_config_id

        return ingestion_config_id

    async def ingest_flow(self, *, flow: Flow, time: datetime, channel_values: dict[str, Any]):
        """
        Ingest a flow.
        """
        # proto_channel_values = []
        rust_channel_values = []
        # Iterate through all expected channels for flow and convert to ingestion types (missing channels use a special empty type)
        for channel in flow.channels:
            val = channel_values.get(channel.name)
            if channel.data_type == ChannelDataType.ENUM and isinstance(val, str):
                for enum_type in channel.enum_types or []:
                    if enum_type.name == val:
                        val = enum_type.key
                        break
            elif channel.data_type == ChannelDataType.BIT_FIELD:
                # TODO: Handle bit field values correctly
                val = None
            # proto_channel_values.append(ChannelDataType.to_ingestion_value(channel.data_type, val))
            rust_channel_values.append(ChannelDataType.to_rust_value(channel.data_type, val))

        print(
            f"Ingesting flow {flow.name} with config id {flow.ingestion_config_id} at {time} with values {rust_channel_values}"
        )
        if not flow.ingestion_config_id:
            raise ValueError(
                "Flow has no ingestion config id -- have you created an ingestion config for this flow?"
            )

        # TODO: Add util function for this.
        ts = time.timestamp()
        secs = int(ts)
        nsecs = int((ts - secs) * 1_000_000_000)
        req = IngestWithConfigDataStreamRequestPy(
            ingestion_config_id=flow.ingestion_config_id,
            flow=flow.name,
            timestamp=TimeValuePy.from_timestamp(secs, nsecs),
            channel_values=rust_channel_values,
        )

        data_queue = Queue()
        data_queue.put(req)
        ingestion_task = asyncio.create_task(self._ingestion_thread(data_queue))
        # None value signals thread to stop.
        # TODO: Make thread task simpler or make queue global. If we make global, do we support multiple sift_streams per-wrapper? Do we manage context in the resource or register a method to put None into the queue atexit?
        data_queue.put(None)
        await ingestion_task

    async def ingest_arbitrary_protobuf_data_stream(
        self,
    ) -> IngestArbitraryProtobufDataStreamResponse:
        """
        Stream arbitrary protobuf data for ingestion.

        Returns:
            The ingestion response.
        """
        raise NotImplementedError("Not implemented")

    def hash(self, asset_name: str, flows: List[Flow]) -> str:
        # TODO:  Taken from sift_py/ingestion/config/telemetry.py. Confirm intent.
        m = hashlib.sha256()
        m.update(asset_name.encode())
        for flow in sorted(flows, key=lambda f: f.name):
            m.update(flow.name.encode())
            # Do not sort channels in alphabetical order since order matters.
            for channel in flow.channels:
                m.update(channel.name.encode())
                # Use api_format for data type since that should be consistent between languages.
                m.update(channel.data_type.as_human_str(api_format=True).encode())
                m.update((channel.description or "").encode())
                # Deprecated.
                m.update((channel.component or "").encode())
                m.update((channel.unit or "").encode())
                for bfe in sorted(channel.bit_field_elements, key=lambda bfe: bfe.index):
                    m.update(bfe.name.encode())
                    m.update(str(bfe.index).encode())
                    m.update(str(bfe.bit_count).encode())
                for enum in sorted(channel.enum_types, key=lambda et: et.key):
                    m.update(str(enum.key).encode())
                    m.update(enum.name.encode())

        return m.hexdigest()
