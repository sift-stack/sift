from __future__ import annotations

import asyncio
import logging
from datetime import datetime, timezone
from math import ceil
from typing import Any, cast

import pandas as pd
from sift.channels.v3.channels_pb2 import (
    GetChannelRequest,
    GetChannelResponse,
    ListChannelsRequest,
    ListChannelsResponse,
)
from sift.channels.v3.channels_pb2_grpc import ChannelServiceStub
from sift.data.v2.data_pb2 import (
    BitFieldValues,
    ChannelQuery,
    GetDataRequest,
    GetDataResponse,
    Query,
)
from sift.data.v2.data_pb2_grpc import DataServiceStub
from sift_py._internal.time import to_timestamp_nanos

from sift_client._internal.low_level_wrappers.base import LowLevelClientBase
from sift_client.transport.grpc_transport import GrpcClient
from sift_client.types.channel import Channel, ChannelDataType

# Configure logging
logger = logging.getLogger(__name__)

CHANNELS_DEFAULT_PAGE_SIZE = 10_000
# TODO: There is a pagination issue API side when requesting multiple channels in single request.
# If all data points for all channels in a single request don't fit into a single page, then
# paging seems to omit all but a single channel. We can increase this batch size once that issue
# has been resolved. In the mean time each channel gets its own request.
REQUEST_BATCH_SIZE = 1


class ChannelsLowLevelClient(LowLevelClientBase):
    """
    Low-level client for the ChannelsAPI.

    This class provides a thin wrapper around the autogenerated bindings for the ChannelsAPI.
    """

    channel_cache = {
        "name_id_map": {},
    }

    def __init__(self, grpc_client: GrpcClient):
        """
        Initialize the ChannelsLowLevelClient.

        Args:
            grpc_client: The gRPC client to use for making API calls.
        """
        self._grpc_client = grpc_client

    def _update_name_id_map(self, channels: list[Channel]):
        """
        Update the name id map with the new channels.
        """
        for channel in channels:
            if channel.bit_field_elements:
                for bit_field_element in channel.bit_field_elements:
                    self.channel_cache["name_id_map"][
                        channel.name + "." + bit_field_element.name
                    ] = channel.id
            self.channel_cache["name_id_map"][channel.name] = channel.id

    async def get_channel(self, channel_id: str) -> Channel:
        """
        Get a channel by channel_id.

        Args:
            channel_id: The channel ID to get.

        Returns:
            The Channel.

        Raises:
            ValueError: If channel_id is not provided.
        """
        if not channel_id:
            raise ValueError("channel_id must be provided")

        request = GetChannelRequest(channel_id=channel_id)
        response = await self._grpc_client.get_stub(ChannelServiceStub).GetChannel(request)
        grpc_channel = cast(GetChannelResponse, response).channel
        channel = Channel._from_proto(grpc_channel)
        self._update_name_id_map([channel])
        return channel

    async def list_channels(
        self,
        *,
        query_filter: str | None = None,
        page_size: int | None = None,
        page_token: str | None = None,
        order_by: str | None = None,
    ) -> tuple[list[Channel], str]:
        """
        List channels with optional filtering and pagination.

        Args:
            query_filter: A CEL filter string.
            page_size: The maximum number of channels to return.
            page_token: A page token for pagination.
            order_by: How to order the retrieved channels.

        Returns:
            A tuple of (channels, next_page_token).
        """

        request_kwargs: dict[str, Any] = {}
        if filter:
            request_kwargs["filter"] = query_filter
        if order_by:
            request_kwargs["order_by"] = order_by
        if page_size:
            request_kwargs["page_size"] = page_size
        if page_token:
            request_kwargs["page_token"] = page_token

        request = ListChannelsRequest(**request_kwargs)
        response = await self._grpc_client.get_stub(ChannelServiceStub).ListChannels(request)
        response = cast(ListChannelsResponse, response)

        channels = [Channel._from_proto(channel) for channel in response.channels]
        self._update_name_id_map(channels)
        return channels, response.next_page_token

    async def list_all_channels(
        self,
        *,
        query_filter: str | None = None,
        order_by: str | None = None,
        max_results: int | None = None,
    ) -> list[Channel]:
        """
        List all channels with optional filtering.

        Args:
            query_filter: A CEL filter string.
            order_by: How to order the retrieved channels.
            max_results: Maximum number of results to return.

        Returns:
            A list of all matching channels.
        """
        # Channels default page size is 10,000 so lower it if we're passing max_results
        page_size = None
        if max_results is not None and max_results <= CHANNELS_DEFAULT_PAGE_SIZE:
            page_size = max_results
        return await self._handle_pagination(
            self.list_channels,
            kwargs={"query_filter": query_filter},
            page_size=page_size,
            order_by=order_by,
            max_results=max_results,
        )

    # TODO: Cache calls. Only read cache if end_time is more than 30 min in the past.
    #       Also, consider manually caching full channel data and evaluating start/end times while ignoring pagination. Do this ful caching at a higher  level though to handle case where pagination fails.
    async def _get_data_impl(
        self,
        *,
        channel_ids: list[str],
        run_id: str | None = None,
        start_time: datetime | None = None,
        end_time: datetime,
        page_size: int | None = None,
        page_token: str | None = None,
        order_by: str | None = None,
    ) -> tuple[list[Any], str | None]:
        """
        Get the data for a channel during a run.
        """
        queries = [
            Query(channel=ChannelQuery(channel_id=channel_id, run_id=run_id))
            for channel_id in channel_ids
        ]
        request_kwargs: dict[str, Any] = {
            "queries": queries,
            "sample_ms": 0,
            "start_time": start_time,
            "end_time": end_time,
            "page_size": page_size,
            "page_token": page_token,
        }

        request = GetDataRequest(**request_kwargs)
        response = await self._grpc_client.get_stub(DataServiceStub).GetData(request)
        response = cast(GetDataResponse, response)
        return response.data, response.next_page_token

    def __filter_cached_channels(self, channel_ids: list[str]) -> list[str]:
        cached_channels = []
        not_cached_channels = []
        for id in channel_ids:
            if self.channel_cache.get(id):
                cached_channels.append(id)
            else:
                not_cached_channels.append(id)
        return cached_channels, not_cached_channels

    def __check_cache(
        self, channel_id: str, start_time: datetime, end_time: datetime, run_id: str | None = None
    ) -> bool:
        """
        Check if the data for a channel during a run is cached.

        Returns:
            A tuple of (data, start_time, end_time)
            where data is a pandas dataframe, start_time is the start time of the data, and end_time is the end time of the data.
        """
        cached_data = self.channel_cache.get(channel_id)
        ret_start_time = start_time
        start_time = start_time or datetime.fromtimestamp(0, tz=timezone.utc)
        ret_end_time = end_time
        ret_data = None
        if cached_data:
            start_time_cached = cached_data.get("start_time")
            end_time_cached = cached_data.get("end_time")
            ret_data = cached_data.get("data")
            # Filter data to desiredtime range
            ret_data = ret_data[start_time:end_time]

            if start_time_cached <= start_time:
                # Cache data starts before the desired time range.
                if start_time < end_time_cached:
                    if end_time <= end_time_cached:
                        # Cache data fully encompasses the desired time range.
                        ret_start_time = None
                        ret_end_time = None
                    else:
                        ret_start_time = end_time_cached
                        ret_end_time = end_time
                else:
                    # Falls outside of cached data range.
                    return (None, None, None)
            else:
                # Cache data starts after the desired time range.
                if start_time_cached < end_time and end_time <= end_time_cached:
                    ret_start_time = start_time
                    ret_end_time = start_time_cached
                else:
                    # Falls outside of cached data range OR fully encompasses the cache data in which case it's also easiest to just call everything.
                    return (None, None, None)

        return (ret_data, ret_start_time, ret_end_time)

    def __update_cache(
        self,
        channel_data: dict[str, pd.DataFrame],
        start_time: datetime,
        end_time: datetime,
        run_id: str | None = None,
    ):
        """
        Update the cache with the new data.
        """
        name_id_map = self.channel_cache.get("name_id_map", {})

        for channel_name, data in channel_data.items():
            channel_id = name_id_map.get(channel_name)
            if not channel_id:
                raise ValueError(
                    f"{channel_name} not found in name_id_map. Not sure got data for this channel without a call that should've updated the map."
                )

            suggested_start_time = (
                datetime.fromtimestamp(0, tz=timezone.utc) if start_time is None else start_time
            )
            if start_time is None and run_id:
                # No start time is treated as epoch time.
                # However, if we passed a run id there may still be data before the run started so we can only know we've gotten data up to run start.
                suggested_start_time = data.index[0]

            if channel_id in self.channel_cache:
                self.channel_cache[channel_id]["data"] = (
                    pd.concat([self.channel_cache[channel_id]["data"], data])
                    .groupby(level=0)
                    .last()
                )
                self.channel_cache[channel_id]["start_time"] = (
                    suggested_start_time
                    if suggested_start_time <= self.channel_cache[channel_id]["start_time"]
                    else self.channel_cache[channel_id]["start_time"]
                )
                self.channel_cache[channel_id]["end_time"] = (
                    end_time
                    if end_time >= self.channel_cache[channel_id]["end_time"]
                    else self.channel_cache[channel_id]["end_time"]
                )
            else:
                self.channel_cache[channel_id] = {
                    "data": data,
                    "start_time": suggested_start_time,
                    "end_time": end_time,
                }

    async def get_channel_data(
        self,
        *,
        channel_ids: list[str],
        run_id: str | None = None,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
        limit: int | None = None,
        ignore_cache: bool = False,
    ):
        """
        Get the data for a channel during a run.
        """
        ret_data = {}
        channel_ids = list(set(channel_ids))
        # No data will be returned if end_time is not provided.
        end_time = end_time or datetime.now(timezone.utc)

        cached_channels, not_cached_channels = (
            ([], channel_ids) if ignore_cache else self.__filter_cached_channels(channel_ids)
        )

        tasks = []
        page_size = limit if limit and limit < 1000 else 1000
        limit = ceil(limit / page_size) if limit else None
        # Queue up calls for non-cached channels in batches.
        batch_size = REQUEST_BATCH_SIZE
        for i in range(0, len(not_cached_channels), batch_size):
            batch = not_cached_channels[i : i + batch_size]
            task = asyncio.create_task(
                self._handle_pagination(
                    self._get_data_impl,
                    kwargs={
                        "channel_ids": batch,
                        "run_id": run_id,
                        "start_time": start_time,
                        "end_time": end_time,
                    },
                    page_size=page_size,
                    max_results=limit,
                )
            )
            tasks.append(task)

        # Handling cached channels 1 by 1 instead of in batches to account for channels that may have been cached from calls with different start/end times.
        for channel_id in cached_channels:
            cached_data, new_start_time, new_end_time = self.__check_cache(
                channel_id, start_time, end_time, run_id
            )

            if cached_data is not None:
                for name in cached_data.columns:
                    ret_data[name] = cached_data
                if new_start_time is None:
                    # Cache fully encompassed the desired time range so don't queue a call.
                    continue
            task = asyncio.create_task(
                self._handle_pagination(
                    self._get_data_impl,
                    kwargs={
                        "channel_ids": [channel_id],
                        "run_id": run_id,
                        "start_time": new_start_time,
                        "end_time": new_end_time or end_time,
                    },
                    page_size=page_size,
                    max_results=limit,
                )
            )
            tasks.append(task)

        pages = await asyncio.gather(*tasks)
        # Flatten the data
        for page in pages:
            for data in page:
                page_results = ChannelsLowLevelClient.try_deserialize_channel_data(data)
                for name, df in page_results.items():
                    if name not in ret_data:
                        ret_data[name] = df
                    else:
                        ret_data[name] = pd.concat([ret_data[name], df]).groupby(level=0).last()

        self.__update_cache(ret_data, start_time, end_time, run_id)

        return ret_data

    @staticmethod
    def try_deserialize_channel_data(channel_data: Any) -> dict[str, pd.DataFrame]:
        """
        Deserialize a channel data object into a numpy array.
        """
        data_type = ChannelDataType.from_str(channel_data.type_url)
        if data_type is None:
            raise ValueError(f"Unknown data type: {channel_data.type_url}")

        proto_data_class = ChannelDataType.proto_data_class(data_type)
        proto_data_value = proto_data_class.FromString(channel_data.value)
        metadata = proto_data_value.metadata
        time_column = []
        value_column = []
        ret_data = {}

        components = (
            proto_data_value.values if proto_data_class == BitFieldValues else [proto_data_value]
        )
        for component in components:
            name = metadata.channel.name
            if proto_data_class == BitFieldValues:
                name += "." + component.name
            for value_obj in component.values:
                time_column.append(to_timestamp_nanos(value_obj.timestamp))
                value_column.append(value_obj.value)
            df = pd.DataFrame({name: value_column}, index=time_column)
            ret_data[name] = df

        return ret_data
