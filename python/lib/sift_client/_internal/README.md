# Sift Client Internal Architecture

This directory contains internal implementation details for the Sift Client library. These components are not part of
the public API and may change without notice.

## Client Structure and Resources

The Sift Client library is structured in a layered architecture:

1. **Transport Layer** (`transport/`):
    - Handles low-level communication with Sift services
    - Implements both gRPC and REST transport options
    - Uses existing gRPC and REST code from `sift_py` (can be migrated once `sift_py` is deprecated)

2. **Low-Level Wrappers** (`_internal/low_level_wrappers/`):
    - Pure async implementations that wrap gRPC stubs
    - Direct mapping to gRPC services
    - Handles serialization/deserialization between protocol buffers and Python objects
    - Not intended for direct use by end users
    - Intended to be as simple as possible and focus on verbose serialization/deserialization

3. **Sift Types** (`types/`):
    - Defines Pydantic models for all domain objects
    - Provides conversion between protocol buffers and Python objects
    - Implements update models for partial updates with field masks

4. **High-Level APIs** (`resources/`):
    - Both async (`AssetsAPIAsync`) and sync (`AssetsAPI`) versions
    - User-friendly interfaces
    - Pythonic methods that abstract away the complexity of the underlying gRPC calls
    - Developer implements Async API and generates Sync API use `generate_sync_api`

5. **Client** (`client.py`):
    - Main entry point for users
    - Provides access to all resource APIs
    - Manages configuration and authentication
    - Allows users to use either async or sync APIs (all handled internally)

## High-Level API Implementation and Generation

### Async API Implementation

High-level APIs are implemented as async classes first:

1. Each resource type (Assets, Runs, etc.) has its own API class (e.g., `AssetsAPIAsync`)
2. These classes inherit from `ResourceBase` which provides common functionality
3. They use low-level clients to make the actual gRPC calls
4. They handle parameter validation, type conversion, and error handling
5. They return Pydantic models instead of raw protocol buffer objects

### Sync API Generation

Synchronous versions of the APIs are automatically generated from the async versions:

1. The `generate_sync_api` function in `_internal/sync_wrapper.py` creates sync versions of async classes
2. It wraps each async method with a sync method that runs the async method in a dedicated event loop
3. It preserves method signatures, docstrings, and type hints
4. The generated sync classes are registered in the `_registered` list for later use in type stub generation

Example from `resources/sync_stubs/__init__.py`:

```python
from sift_client._internal.sync_wrapper import generate_sync_api
from sift_client.resources import AssetsAPIAsync, PingAPIAsync

PingAPI = generate_sync_api(PingAPIAsync, "PingAPI")
AssetsAPI = generate_sync_api(AssetsAPIAsync, "AssetsAPI")
```

### Type Stub Generation

Type stubs (.pyi files) are automatically generated for the sync APIs:

1. The `gen_pyi.py` script generates .pyi files for the sync APIs
2. It uses the `_registered` list to find classes generated by `generate_sync_api`
3. It extracts method signatures, docstrings, and type hints from the sync classes
4. It generates type stubs that provide proper type checking for the sync APIs

This process ensures that both the async and sync APIs have proper type checking support.

## Type System and Update Models

### Base Type System

The type system is built on Pydantic models:

1. `BaseType` in `types/base.py` is the base class for all domain objects
2. It provides conversion between protocol buffers and Python objects
3. It attaches the client to the object for method chaining
4. It's immutable (frozen) to prevent accidental modification

### Update Models

Update models are used for partial updates with field masks:

1. `ModelUpdate` in `types/base.py` is the base class for all update models
2. It provides conversion to protocol buffers with field masks
3. It only includes explicitly set fields in the update

### Specific Types

Specific types inherit from `BaseType` and implement domain-specific logic:

1. Each domain object (Asset, Run, etc.) has its own type class
2. These classes implement the `_from_proto` method to convert from protocol buffers
3. They define properties and methods specific to the domain object
4. They may have convenience methods that use the attached client

## Event Loop Architecture

The Sift Client uses a dedicated event loop architecture for handling asynchronous gRPC operations. This document
explains the design decisions and implications of this approach.

### Overview

The architecture consists of:

1. **Low-level clients**: Pure async implementations that wrap gRPC stubs
2. **High-level APIs**: Both sync and async versions that provide user-friendly interfaces
3. **Dedicated event loop**: A separate event loop in its own thread for gRPC async operations if needed for high-level API synchronous calls

### Design Decisions

#### Why a Dedicated Event Loop?

The gRPC async client requires that all operations (creating stubs and making calls) happen in the same event loop
context. When using the client in different contexts (like inside `asyncio.run()` or in a Jupyter notebook), this can
lead to the "Task got Future attached to a different loop" error.

To solve this, we create a dedicated event loop in a separate thread specifically for gRPC operations. This ensures
that:

1. All gRPC async stubs are created in the same event loop
2. All gRPC async operations run in that same event loop
3. The client works reliably in any context (sync or async)

When using within an async context, the dedicated event loop is not used and the client uses the default event loop.

See `sift_client/transport/grpc_transport:GrpcClient.get_stub` for more details on event loop management.

#### Low-Level vs High-Level APIs

- **Low-level clients** (`_internal/low_level_wrappers/`):
    - Pure async implementations
    - Direct mapping to gRPC services
    - Not intended for direct use by end users

- **High-level APIs** (`resources/`):
    - Both sync (`PingAPI`) and async (`PingAPIAsync`) versions
    - User-friendly interfaces with proper error handling
    - The sync version internally manages event loops to work with async low-level clients

### Implications

#### Performance Considerations

Using a dedicated event loop in a separate thread has some performance implications:

1. **Thread Overhead**: Additional memory and CPU usage for the extra thread
2. **Synchronization Cost**: Communication between event loops requires thread synchronization
3. **Concurrency Benefit**: gRPC operations can run concurrently with the main application

However, this trade-off is often worth it for the ease of maintenance and development.

#### Usage in Different Contexts

1. **In a synchronous context**:
    - The sync API (`PingAPI`) handles all event loop management internally
    - Users don't need to worry about async/await or event loops

2. **In an asynchronous context**:
    - Users can use the async API (`PingAPIAsync`) directly with `await`
    - Operations still execute in the dedicated gRPC event loop, not in the caller's loop

3. **In a Jupyter notebook or interactive environment**:
    - The dedicated event loop approach ensures reliable operation
    - No "loop already running" errors when using the sync API

### Cleanup and Resource Management

The `GrpcClient` class handles proper cleanup of resources:

1. It registers an `atexit` handler to ensure channels are closed
2. It implements context manager protocol for both sync and async usage
3. It properly stops the dedicated event loop and joins the thread on cleanup

### Alternative Approaches Considered

1. **Event Loop Sharing**: Using the caller's event loop if one exists
    - More efficient but requires careful management
    - Difficult to ensure reliability in all contexts

2. **Fully Synchronous Low-Level API**: Using only sync gRPC stubs
    - Would require duplicating code for async versions

The dedicated event loop approach was chosen as the best balance between reliability, performance, and code
maintainability.
