{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b202351",
   "metadata": {},
   "source": [
    "# Sift Client Ingestion\n",
    "\n",
    "This notebook demonstrates features of SiftClient ingestion, from basic usage to advanced patterns.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Important**: The ingestion streaming client requires the `sift-stream` optional dependency. Install it with:\n",
    "\n",
    "```bash\n",
    "pip install sift-stack-py[sift-stream]\n",
    "```\n",
    "\n",
    "## Topics Covered\n",
    "\n",
    "1. **Basic Example**: Simple single flow sending using FlowConfig\n",
    "2. **Advanced FlowBuilderPy**: High-performance flow building with direct run ID management\n",
    "3. **High-Performance Batch Sending**: Efficiently sending multiple flows using FlowBuilderPy\n",
    "4. **Queue-Based Lazy Flow Creation**: Dynamic flow registration with multi-task architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324ed85",
   "metadata": {},
   "source": [
    "## 1. Basic Example: Sending Individual Flows with FlowConfig\n",
    "\n",
    "This example shows the simplest way to send telemetry data to Sift:\n",
    "- Create an ingestion config with flow definitions\n",
    "- Save the flow config from the ingestion config (no API call needed)\n",
    "- Create a run to associate data with\n",
    "- Send individual flows one at a time using `as_flow()`\n",
    "\n",
    "This is the simplest approach and is recommended for basic use cases where performance is not critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b75dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from sift_client import SiftClient, SiftConnectionConfig\n",
    "from sift_client.sift_types import (\n",
    "    ChannelConfig,\n",
    "    ChannelDataType,\n",
    "    FlowConfig,\n",
    "    IngestionConfigCreate,\n",
    "    RunCreate,\n",
    ")\n",
    "\n",
    "\n",
    "async def basic_example():\n",
    "    # Configure connection to Sift\n",
    "    connection_config = SiftConnectionConfig(\n",
    "        api_key=\"my_api_key\",\n",
    "        grpc_url=\"sift_grpc_url\",\n",
    "        rest_url=\"sift_rest_url\",\n",
    "    )\n",
    "\n",
    "    client = SiftClient(connection_config=connection_config)\n",
    "\n",
    "    # Define your telemetry schema using an flow config and ingestion config\n",
    "    flow_config = FlowConfig(\n",
    "                name=\"onboard_sensors\",\n",
    "                channels=[\n",
    "                    ChannelConfig(name=\"motor_temp\", unit=\"C\", data_type=ChannelDataType.DOUBLE),\n",
    "                    ChannelConfig(\n",
    "                        name=\"tank_pressure\", unit=\"kPa\", data_type=ChannelDataType.DOUBLE\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    ingestion_config = IngestionConfigCreate(\n",
    "        asset_name=\"sift_rover_1\",\n",
    "        flows=[flow_config],\n",
    "    )\n",
    "\n",
    "    # Create a run to associate this data collection session\n",
    "    run = RunCreate(name=\"sift_rover-\" + str(int(time.time())))\n",
    "\n",
    "    # Create the streaming client\n",
    "    async with await client.async_.ingestion.create_ingestion_config_streaming_client(\n",
    "        ingestion_config=ingestion_config,\n",
    "        run=run,\n",
    "    ) as ingest_client:\n",
    "        # Send data in a loop\n",
    "        for i in range(10):\n",
    "            # Create a flow with timestamp and values using the saved flow_config\n",
    "            # The timestamp can also be left out to default to datetime.now(timezone.utc)\n",
    "            flow = flow_config.as_flow(\n",
    "                timestamp=datetime.now(timezone.utc),\n",
    "                values={\n",
    "                    \"motor_temp\": 50.0 + random.random() * 5.0,\n",
    "                    \"tank_pressure\": 2000.0 + random.random() * 100.0,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # Send the flow to Sift\n",
    "            await ingest_client.send(flow=flow)\n",
    "\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# asyncio.run(basic_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf10f3",
   "metadata": {},
   "source": [
    "## 2. Advanced FlowBuilderPy Usage\n",
    "\n",
    "This example demonstrates the advanced `FlowBuilderPy` paradigm, which provides better performance and more control:\n",
    "- Get `FlowDescriptorPy` using `get_flow_descriptor()`\n",
    "- Retrieve the run ID from SiftStream using `get_run_id()`\n",
    "- Create `FlowBuilderPy` from the descriptor\n",
    "- Set the run ID directly on the flow builder using `attach_run_id()`\n",
    "- Use channel indices from the descriptor mapping to avoid hash operations\n",
    "- Use `set()` with channel indices instead of `set_with_key()` for maximum performance\n",
    "- Build the request and send using `send_requests()` or `send_requests_nonblocking()`\n",
    "\n",
    "**Note**: This approach requires managing the run ID directly, making it more advanced but also more performant. Using channel indices instead of channel names avoids hash lookups, providing the best performance for high-frequency data sending. It's useful when you need fine-grained control or are sending data for multiple runs/assets with a single SiftStream instance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aec10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def advanced_flowbuilder_example():\n",
    "    \"\"\"Example showing advanced FlowBuilderPy usage with channel indices for maximum performance.\"\"\"\n",
    "    from sift_stream_bindings import FlowBuilderPy, TimeValuePy, ValuePy, ChannelIndexPy\n",
    "\n",
    "    connection_config = SiftConnectionConfig(\n",
    "        api_key=\"my_api_key\",\n",
    "        grpc_url=\"sift_grpc_url\",\n",
    "        rest_url=\"sift_rest_url\",\n",
    "    )\n",
    "\n",
    "    client = SiftClient(connection_config=connection_config)\n",
    "\n",
    "    ingestion_config = IngestionConfigCreate(\n",
    "        asset_name=\"sift_rover_1\",\n",
    "        flows=[\n",
    "            FlowConfig(\n",
    "                name=\"onboard_sensors\",\n",
    "                channels=[\n",
    "                    ChannelConfig(name=\"motor_temp\", unit=\"C\", data_type=ChannelDataType.DOUBLE),\n",
    "                    ChannelConfig(\n",
    "                        name=\"tank_pressure\", unit=\"kPa\", data_type=ChannelDataType.DOUBLE\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    run = RunCreate(name=\"sift_rover-\" + str(int(time.time())))\n",
    "\n",
    "    async with await client.async_.ingestion.create_ingestion_config_streaming_client(\n",
    "        ingestion_config=ingestion_config,\n",
    "        run=run,\n",
    "    ) as ingest_client:\n",
    "        # Get the flow descriptor and run ID from SiftStream\n",
    "        descriptor = ingest_client.get_flow_descriptor(flow_name=\"onboard_sensors\")\n",
    "        run_id = ingest_client.get_run_id()\n",
    "\n",
    "        if run_id is None:\n",
    "            raise ValueError(\"Run ID is required for FlowBuilderPy usage\")\n",
    "\n",
    "        # Get the mapping from channel names to ChannelIndexPy\n",
    "        # This allows us to avoid hash lookups by using indices directly\n",
    "        channel_index_map = descriptor.mapping()\n",
    "        \n",
    "        # Pre-compute channel indices and value conversion methods\n",
    "        # This creates a list of (ChannelIndexPy, conversion_method) tuples\n",
    "        # that can be reused for each flow, avoiding hash operations\n",
    "        #\n",
    "        # If this technique is used, caching the indices and conversion method\n",
    "        # is strongly recommended.\n",
    "        channel_indices_and_methods = [\n",
    "            (channel_index_map[\"motor_temp\"], ValuePy.Double),\n",
    "            (channel_index_map[\"tank_pressure\"], ValuePy.Double),\n",
    "        ]\n",
    "\n",
    "        # Send data in a loop using FlowBuilderPy with channel indices\n",
    "        for i in range(10):\n",
    "            # Create a FlowBuilderPy from the descriptor\n",
    "            flow_builder = FlowBuilderPy(descriptor)\n",
    "            \n",
    "            # Attach the run ID directly to the flow builder\n",
    "            flow_builder.attach_run_id(run_id)\n",
    "            \n",
    "            # Set channel values using set() with pre-computed indices\n",
    "            # This avoids hash lookups and provides better performance\n",
    "            motor_temp_value = 50.0 + random.random() * 5.0\n",
    "            tank_pressure_value = 2000.0 + random.random() * 100.0\n",
    "            \n",
    "            # If the raw data class used provides in-order iteration over the raw data, you can also iterate\n",
    "            # over the values and encoding information directly. Since the value indices are used, the\n",
    "            # additional per-channel hash lookup is not needed, further improving performance.\n",
    "            #\n",
    "            # Though for convenience, the values can also be set using set_with_key() which takes a channel name\n",
    "            # and value.\n",
    "            #\n",
    "            # Example:\n",
    "            #\n",
    "            # flow_builder.set_with_key(\"motor_temp\", motor_temp_value)\n",
    "            # flow_builder.set_with_key(\"tank_pressure\", tank_pressure_value)\n",
    "            values = [motor_temp_value, tank_pressure_value]\n",
    "            for (channel_index, conversion_method), value in zip(channel_indices_and_methods, values):\n",
    "                flow_builder.set(channel_index, conversion_method(value))\n",
    "            \n",
    "            # Build the request with current timestamp\n",
    "            request = flow_builder.request(TimeValuePy.now())\n",
    "            \n",
    "            # Send the request (non-blocking version)\n",
    "            ingest_client.send_requests_nonblocking([request])\n",
    "\n",
    "            await asyncio.sleep(0.1)\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# asyncio.run(advanced_flowbuilder_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab425ee7",
   "metadata": {},
   "source": [
    "## 3. High-Performance Batch Sending\n",
    "\n",
    "This example demonstrates high-performance batch sending using `FlowBuilderPy` with channel indices and `send_requests_nonblocking()`:\n",
    "- Pre-compute channel indices from the descriptor mapping to avoid hash operations\n",
    "- Use `FlowBuilderPy` with `set()` and channel indices for maximum performance\n",
    "- Use `send_requests_nonblocking()` for non-blocking batch sending\n",
    "- This approach provides the best performance for high-throughput scenarios\n",
    "\n",
    "The combination of channel indices (avoiding hash lookups) and non-blocking batch sending allows the underlying Rust client to handle batching and sending asynchronously, minimizing Python overhead and maximizing throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def high_performance_batch_example():\n",
    "    \"\"\"Example showing high-performance batch sending with FlowBuilderPy using channel indices.\"\"\"\n",
    "    from datetime import timedelta\n",
    "    from sift_stream_bindings import FlowBuilderPy, TimeValuePy, ValuePy\n",
    "\n",
    "    connection_config = SiftConnectionConfig(\n",
    "        api_key=\"my_api_key\",\n",
    "        grpc_url=\"sift_grpc_url\",\n",
    "        rest_url=\"sift_rest_url\",\n",
    "    )\n",
    "\n",
    "    client = SiftClient(connection_config=connection_config)\n",
    "\n",
    "    ingestion_config = IngestionConfigCreate(\n",
    "        asset_name=\"sift_rover_1\",\n",
    "        flows=[\n",
    "            FlowConfig(\n",
    "                name=\"onboard_sensors\",\n",
    "                channels=[\n",
    "                    ChannelConfig(name=\"motor_temp\", unit=\"C\", data_type=ChannelDataType.DOUBLE),\n",
    "                    ChannelConfig(\n",
    "                        name=\"tank_pressure\", unit=\"kPa\", data_type=ChannelDataType.DOUBLE\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    run = RunCreate(name=\"sift_rover-\" + str(int(time.time())))\n",
    "\n",
    "    async with await client.async_.ingestion.create_ingestion_config_streaming_client(\n",
    "        ingestion_config=ingestion_config,\n",
    "        run=run,\n",
    "    ) as ingest_client:\n",
    "        # Get the flow descriptor and run ID\n",
    "        descriptor = ingest_client.get_flow_descriptor(flow_name=\"onboard_sensors\")\n",
    "        run_id = ingest_client.get_run_id()\n",
    "\n",
    "        if run_id is None:\n",
    "            raise ValueError(\"Run ID is required for FlowBuilderPy usage\")\n",
    "\n",
    "        # Pre-compute channel indices and conversion methods for maximum performance\n",
    "        # This avoids hash lookups when setting values in the loop below\n",
    "        channel_index_map = descriptor.mapping()\n",
    "        channel_indices_and_methods = [\n",
    "            (channel_index_map[\"motor_temp\"], ValuePy.Double),\n",
    "            (channel_index_map[\"tank_pressure\"], ValuePy.Double),\n",
    "        ]\n",
    "\n",
    "        # Generate 5 seconds of data at 10Hz (10 flows per second = 50 flows total)\n",
    "        sample_rate_hz = 10\n",
    "        duration_seconds = 5\n",
    "        num_flows = sample_rate_hz * duration_seconds  # 50 flows\n",
    "\n",
    "        start_time = datetime.now(timezone.utc)\n",
    "        requests = []\n",
    "        \n",
    "        for i in range(num_flows):\n",
    "            # Calculate timestamp for each sample (spaced 0.1 seconds apart)\n",
    "            timestamp_secs = int((start_time + timedelta(seconds=i / sample_rate_hz)).timestamp())\n",
    "            timestamp = TimeValuePy.from_timestamp(timestamp_secs, 0)\n",
    "            \n",
    "            # Create FlowBuilderPy and build request using pre-computed indices\n",
    "            flow_builder = FlowBuilderPy(descriptor)\n",
    "            flow_builder.attach_run_id(run_id)\n",
    "            \n",
    "            # Generate values\n",
    "            motor_temp_value = 50.0 + random.random() * 5.0\n",
    "            tank_pressure_value = 2000.0 + random.random() * 100.0\n",
    "            \n",
    "            # Use indices directly - no hash operations!\n",
    "            values = [motor_temp_value, tank_pressure_value]\n",
    "            for (channel_index, conversion_method), value in zip(channel_indices_and_methods, values):\n",
    "                flow_builder.set(channel_index, conversion_method(value))\n",
    "\n",
    "            request = flow_builder.request(timestamp)\n",
    "            requests.append(request)\n",
    "\n",
    "        # Send all requests in a single non-blocking batch operation\n",
    "        # The combination of channel indices + non-blocking batch sending provides\n",
    "        # the best performance for high-throughput scenarios\n",
    "        ingest_client.send_requests_nonblocking(requests)\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# asyncio.run(high_performance_batch_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8efce39",
   "metadata": {},
   "source": [
    "## 4. Queue-Based Lazy Flow Creation\n",
    "\n",
    "This example demonstrates a multi-task architecture for handling dynamic flow schemas using `add_new_flows()`:\n",
    "- **Task 1**: Ingest raw data from a source and push to Queue 1\n",
    "- **Task 2**: Read from Queue 1, check if flow descriptor is cached\n",
    "  - If not cached, call `add_new_flows()` to register the new flow\n",
    "  - After registration, retrieve the descriptor and cache it\n",
    "  - Push the message with descriptor to Queue 2\n",
    "- **Task 3**: Drain Queue 2, decode the raw data, and send to Sift using `FlowBuilderPy`\n",
    "\n",
    "This pattern enables lazy flow registration, allowing you to handle unknown schemas at runtime without pre-registering all possible flows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8829136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from sift_stream_bindings import FlowBuilderPy, FlowDescriptorPy, TimeValuePy\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawDataMessage:\n",
    "    \"\"\"Represents raw data that needs to be decoded and sent.\"\"\"\n",
    "    flow_name: str\n",
    "    timestamp: datetime\n",
    "    channel_values: dict[str, float]  # Raw channel name -> value mapping\n",
    "\n",
    "\n",
    "async def queue_based_lazy_flow_example():\n",
    "    \"\"\"Example demonstrating queue-based lazy flow creation with add_new_flows.\"\"\"\n",
    "    from sift_client import SiftClient, SiftConnectionConfig\n",
    "    from sift_client.sift_types import (\n",
    "        ChannelConfig,\n",
    "        ChannelDataType,\n",
    "        FlowConfig,\n",
    "        IngestionConfigCreate,\n",
    "        RunCreate,\n",
    "    )\n",
    "\n",
    "    connection_config = SiftConnectionConfig(\n",
    "        api_key=\"my_api_key\",\n",
    "        grpc_url=\"sift_grpc_url\",\n",
    "        rest_url=\"sift_rest_url\",\n",
    "    )\n",
    "\n",
    "    client = SiftClient(connection_config=connection_config)\n",
    "\n",
    "    # Start with an empty ingestion config - flows will be added dynamically\n",
    "    ingestion_config = IngestionConfigCreate(\n",
    "        asset_name=\"sift_rover_1\",\n",
    "        flows=[],  # Empty initially\n",
    "    )\n",
    "\n",
    "    run = RunCreate(name=\"sift_rover-\" + str(int(time.time())))\n",
    "\n",
    "    async with await client.async_.ingestion.create_ingestion_config_streaming_client(\n",
    "        ingestion_config=ingestion_config,\n",
    "        run=run,\n",
    "    ) as ingest_client:\n",
    "        # Queues for the pipeline\n",
    "        queue1: asyncio.Queue[RawDataMessage] = asyncio.Queue()\n",
    "        queue2: asyncio.Queue[tuple[RawDataMessage, FlowDescriptorPy]] = asyncio.Queue()\n",
    "        \n",
    "        # Cache for flow descriptors (flow_name -> FlowDescriptorPy)\n",
    "        descriptor_cache: dict[str, FlowDescriptorPy] = {}\n",
    "        \n",
    "        # Cache for flow configs (flow_name -> FlowConfig)\n",
    "        # In a real scenario, you'd derive this from your raw data schema\n",
    "        flow_config_cache: dict[str, FlowConfig] = {\n",
    "            \"onboard_sensors\": FlowConfig(\n",
    "                name=\"onboard_sensors\",\n",
    "                channels=[\n",
    "                    ChannelConfig(name=\"motor_temp\", unit=\"C\", data_type=ChannelDataType.DOUBLE),\n",
    "                    ChannelConfig(\n",
    "                        name=\"tank_pressure\", unit=\"kPa\", data_type=ChannelDataType.DOUBLE\n",
    "                    ),\n",
    "                ],\n",
    "            ),\n",
    "            \"navigation\": FlowConfig(\n",
    "                name=\"navigation\",\n",
    "                channels=[\n",
    "                    ChannelConfig(name=\"gps_lat\", unit=\"deg\", data_type=ChannelDataType.DOUBLE),\n",
    "                    ChannelConfig(name=\"gps_lon\", unit=\"deg\", data_type=ChannelDataType.DOUBLE),\n",
    "                ],\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        run_id = ingest_client.get_run_id()\n",
    "\n",
    "        # Task 1: Ingest raw data and push to Queue 1\n",
    "        async def ingest_task():\n",
    "            \"\"\"Simulate ingesting raw data from a source.\"\"\"\n",
    "            for i in range(20):\n",
    "                # Simulate different flows arriving\n",
    "                flow_name = \"onboard_sensors\" if i % 2 == 0 else \"navigation\"\n",
    "                \n",
    "                if flow_name == \"onboard_sensors\":\n",
    "                    raw_data = RawDataMessage(\n",
    "                        flow_name=flow_name,\n",
    "                        timestamp=datetime.now(timezone.utc),\n",
    "                        channel_values={\n",
    "                            \"motor_temp\": 50.0 + random.random() * 5.0,\n",
    "                            \"tank_pressure\": 2000.0 + random.random() * 100.0,\n",
    "                        },\n",
    "                    )\n",
    "                else:\n",
    "                    raw_data = RawDataMessage(\n",
    "                        flow_name=flow_name,\n",
    "                        timestamp=datetime.now(timezone.utc),\n",
    "                        channel_values={\n",
    "                            \"gps_lat\": 37.7749 + random.random() * 0.01,\n",
    "                            \"gps_lon\": -122.4194 + random.random() * 0.01,\n",
    "                        },\n",
    "                    )\n",
    "                \n",
    "                queue1.put_nowait(raw_data)\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "        # Task 2: Register flows lazily\n",
    "        async def registration_task():\n",
    "            \"\"\"Check if flow is registered, register if needed, then push to Queue 2.\"\"\"\n",
    "            while True:\n",
    "                try:\n",
    "                    raw_data = await asyncio.wait_for(queue1.get(), timeout=1.0)\n",
    "                except asyncio.TimeoutError:\n",
    "                    # Check if ingest_task is done by checking queue size\n",
    "                    if queue1.empty():\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                flow_name = raw_data.flow_name\n",
    "                \n",
    "                # Check if descriptor is cached\n",
    "                if flow_name not in descriptor_cache:\n",
    "                    \n",
    "                    # For this example, the flow configs are pre-defined above. \n",
    "                    # \n",
    "                    # Though in practice, these would often be dynamically generated based on\n",
    "                    # the raw data schema.\n",
    "                    if flow_name not in flow_config_cache:\n",
    "                        raise ValueError(f\"Flow config not found for {flow_name}\")\n",
    "                    \n",
    "                    flow_config = flow_config_cache[flow_name]\n",
    "        \n",
    "                    # Convert to Rust FlowConfigPy format\n",
    "                    from sift_stream_bindings import FlowConfigPy, ChannelConfigPy, ChannelDataTypePy\n",
    "                    \n",
    "                    channel_configs_py = [\n",
    "                        ChannelConfigPy(\n",
    "                            name=ch.name,\n",
    "                            data_type=ChannelDataTypePy.Double if ch.data_type == ChannelDataType.DOUBLE else ChannelDataTypePy.Double,\n",
    "                            unit=ch.unit,\n",
    "                            description=ch.description or \"\",\n",
    "                            enum_types=[],\n",
    "                            bit_field_elements=[],\n",
    "                        )\n",
    "                        for ch in flow_config.channels\n",
    "                    ]\n",
    "                    \n",
    "                    flow_config_py = FlowConfigPy(\n",
    "                        name=flow_config.name,\n",
    "                        channels=channel_configs_py,\n",
    "                    )\n",
    "                    \n",
    "                    # Register the new flow\n",
    "                    await ingest_client.add_new_flows([flow_config_py])\n",
    "                    \n",
    "                    # Get the descriptor and cache it\n",
    "                    descriptor = ingest_client.get_flow_descriptor(flow_name)\n",
    "                    descriptor_cache[flow_name] = descriptor\n",
    "                    print(f\"Registered new flow: {flow_name}\")\n",
    "                \n",
    "                # Push to Queue 2 with the descriptor\n",
    "                await queue2.put((raw_data, descriptor_cache[flow_name]))\n",
    "\n",
    "        # Task 3: Decode and send\n",
    "        async def send_task():\n",
    "            \"\"\"Decode raw data and send to Sift using FlowBuilderPy.\"\"\"\n",
    "            while True:\n",
    "                try:\n",
    "                    raw_data, descriptor = await asyncio.wait_for(queue2.get(), timeout=1.0)\n",
    "                except asyncio.TimeoutError:\n",
    "                    # Check if registration_task is done\n",
    "                    if queue2.empty() and queue1.empty():\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                # Create FlowBuilderPy and set values\n",
    "                flow_builder = FlowBuilderPy(descriptor)\n",
    "                flow_builder.attach_run_id(run_id)\n",
    "                \n",
    "                # Set all channel values from raw data.\n",
    "                for channel_name, value in raw_data.channel_values.items():\n",
    "                    flow_builder.set_with_key(channel_name, value)\n",
    "                \n",
    "                # Convert timestamp to TimeValuePy\n",
    "                timestamp_secs = int(raw_data.timestamp.timestamp())\n",
    "                timestamp = TimeValuePy.from_timestamp(timestamp_secs, 0)\n",
    "                \n",
    "                # Build request and send\n",
    "                request = flow_builder.request(timestamp)\n",
    "                await ingest_client.send_requests([request])\n",
    "\n",
    "        # Run all tasks concurrently\n",
    "        await asyncio.gather(\n",
    "            ingest_task(),\n",
    "            registration_task(),\n",
    "            send_task(),\n",
    "        )\n",
    "\n",
    "\n",
    "# Uncomment to run:\n",
    "# asyncio.run(queue_based_lazy_flow_example())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "title": "Sift Client Ingestion"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
