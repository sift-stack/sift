# Generated by the protocol buffer compiler.  DO NOT EDIT!
# sources: sift/data_imports/v2/data_imports.proto
# plugin: python-betterproto
# This file has been @generated

from dataclasses import dataclass
from datetime import datetime
from typing import (
    TYPE_CHECKING,
    Dict,
    List,
    Optional,
)

import betterproto
import grpclib
from betterproto.grpc.grpclib_server import ServiceBase

from ...common.type import v1 as __common_type_v1__


if TYPE_CHECKING:
    import grpclib.server
    from betterproto.grpc.grpclib_client import MetadataLike
    from grpclib.metadata import Deadline


class TimeFormat(betterproto.Enum):
    UNSPECIFIED = 0
    RELATIVE_NANOSECONDS = 1
    RELATIVE_MICROSECONDS = 2
    RELATIVE_MILLISECONDS = 3
    RELATIVE_SECONDS = 4
    RELATIVE_MINUTES = 5
    RELATIVE_HOURS = 6
    ABSOLUTE_RFC3339 = 10
    ABSOLUTE_DATETIME = 11
    ABSOLUTE_UNIX_SECONDS = 12
    ABSOLUTE_UNIX_MILLISECONDS = 13
    ABSOLUTE_UNIX_MICROSECONDS = 14
    ABSOLUTE_UNIX_NANOSECONDS = 15


class DataImportStatus(betterproto.Enum):
    UNSPECIFIED = 0
    PENDING = 1
    IN_PROGRESS = 2
    SUCCEEDED = 3
    FAILED = 4


@dataclass(eq=False, repr=False)
class CreateDataImportFromUrlRequest(betterproto.Message):
    url: str = betterproto.string_field(1)
    """
    The url to import. HTTP and S3 urls are supported.
     If you need to import non-public S3 objects, please contact Sift to set that up.
    """

    csv_config: "CsvConfig" = betterproto.message_field(2)
    ch10_config: "Ch10Config" = betterproto.message_field(3)
    tdms_config: "TdmsConfig" = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class CreateDataImportFromUrlResponse(betterproto.Message):
    data_import_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class GetDataImportRequest(betterproto.Message):
    data_import_id: str = betterproto.string_field(1)


@dataclass(eq=False, repr=False)
class GetDataImportResponse(betterproto.Message):
    data_import: "DataImport" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class CreateDataImportFromUploadRequest(betterproto.Message):
    csv_config: "CsvConfig" = betterproto.message_field(1)
    ch10_config: "Ch10Config" = betterproto.message_field(3)
    tdms_config: "TdmsConfig" = betterproto.message_field(4)


@dataclass(eq=False, repr=False)
class CreateDataImportFromUploadResponse(betterproto.Message):
    upload_url: str = betterproto.string_field(1)
    data_import_id: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class CsvConfig(betterproto.Message):
    asset_name: str = betterproto.string_field(1)
    run_name: str = betterproto.string_field(2)
    run_id: str = betterproto.string_field(3)
    """
    The id of the run to add this data to. If set, `run_name` is ignored.
    """

    first_data_row: int = betterproto.uint32_field(4)
    """
    The first row to start reading as data. Can be used to skip header rows.
     The first row in the file is 1.
    """

    time_column: Optional["CsvTimeColumn"] = betterproto.message_field(5, optional=True)
    data_columns: Dict[int, "__common_type_v1__.ChannelConfig"] = betterproto.map_field(
        6, betterproto.TYPE_UINT32, betterproto.TYPE_MESSAGE
    )
    """
    A map from column number (1-indexed) to the channel configuration for that column.
    """


@dataclass(eq=False, repr=False)
class CsvTimeColumn(betterproto.Message):
    column_number: int = betterproto.uint32_field(1)
    """The column number (1-indexed) of the time column."""

    format: "TimeFormat" = betterproto.enum_field(2)
    relative_start_time: Optional[datetime] = betterproto.message_field(
        3, optional=True
    )


@dataclass(eq=False, repr=False)
class DetectConfigRequest(betterproto.Message):
    data: bytes = betterproto.bytes_field(1)


@dataclass(eq=False, repr=False)
class DetectConfigResponse(betterproto.Message):
    csv_config: "CsvConfig" = betterproto.message_field(1)


@dataclass(eq=False, repr=False)
class Ch10Config(betterproto.Message):
    asset_name: str = betterproto.string_field(1)
    run_name: str = betterproto.string_field(2)
    scale_values: bool = betterproto.bool_field(3)


@dataclass(eq=False, repr=False)
class TdmsConfig(betterproto.Message):
    asset_name: str = betterproto.string_field(1)
    run_name: str = betterproto.string_field(2)
    start_time_override: datetime = betterproto.message_field(3)
    """
    Override the wf_start_time metadata field for all channels.
     Useful if your waveform channels have wf_increment but no wf_start_time (Veristand is guilty of this).
    """


@dataclass(eq=False, repr=False)
class DataImport(betterproto.Message):
    data_import_id: str = betterproto.string_field(1)
    source_url: str = betterproto.string_field(3)
    status: "DataImportStatus" = betterproto.enum_field(4)
    error_message: str = betterproto.string_field(5)
    created_date: datetime = betterproto.message_field(7)
    modified_date: datetime = betterproto.message_field(8)
    csv_config: "CsvConfig" = betterproto.message_field(6)
    ch10_config: "Ch10Config" = betterproto.message_field(9)
    tdms_config: "TdmsConfig" = betterproto.message_field(10)


@dataclass(eq=False, repr=False)
class ListDataImportsRequest(betterproto.Message):
    page_size: int = betterproto.uint32_field(1)
    """
    The maximum number of data imports to return. The service may return fewer than this value.
     If unspecified, at most 50 data imports will be returned. The maximum value is 1000; values above
     1000 will be coerced to 1000. Optional.
    """

    page_token: str = betterproto.string_field(2)
    """
    A page token, received from a previous `ListDataImports` call.
     Provide this to retrieve the subsequent page.
     When paginating, all other parameters provided to `ListDataImports` must match
     the call that provided the page token. Optional.
    """

    filter: str = betterproto.string_field(3)
    """
    A [Common Expression Language (CEL)](https://github.com/google/cel-spec) filter string.
     Available fields to filter by are `data_import_id`, `source_url`, `status`.
     For further information about how to use CELs, please refer to [this guide](https://github.com/google/cel-spec/blob/master/doc/langdef.md#standard-definitions).
    """

    order_by: str = betterproto.string_field(4)
    """
    How to order the retrieved data imports. Formatted as a comma-separated string i.e. "FIELD_NAME[ desc],...".
     Available fields to order_by are `created_date` and `modified_date`.
     If left empty, items are ordered by `created_date` in ascending order (oldest-first).
     For more information about the format of this field, read [this](https://google.aip.dev/132#ordering)
     Example: "created_date desc,modified_date"
    """


@dataclass(eq=False, repr=False)
class ListDataImportsResponse(betterproto.Message):
    data_imports: List["DataImport"] = betterproto.message_field(1)
    next_page_token: str = betterproto.string_field(2)


@dataclass(eq=False, repr=False)
class RetryDataImportRequest(betterproto.Message):
    data_import_id: str = betterproto.string_field(1)
    """
    data_import_id is the id of the data import to retry.
     You can only retry an import that is a "url" based import (created with CreateDataImportFromUrl) and is in a failed state.
    """


@dataclass(eq=False, repr=False)
class RetryDataImportResponse(betterproto.Message):
    pass


class DataImportServiceStub(betterproto.ServiceStub):
    async def create_data_import_from_url(
        self,
        create_data_import_from_url_request: "CreateDataImportFromUrlRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "CreateDataImportFromUrlResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/CreateDataImportFromUrl",
            create_data_import_from_url_request,
            CreateDataImportFromUrlResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def create_data_import_from_upload(
        self,
        create_data_import_from_upload_request: "CreateDataImportFromUploadRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "CreateDataImportFromUploadResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/CreateDataImportFromUpload",
            create_data_import_from_upload_request,
            CreateDataImportFromUploadResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def detect_config(
        self,
        detect_config_request: "DetectConfigRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "DetectConfigResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/DetectConfig",
            detect_config_request,
            DetectConfigResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def list_data_imports(
        self,
        list_data_imports_request: "ListDataImportsRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "ListDataImportsResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/ListDataImports",
            list_data_imports_request,
            ListDataImportsResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def retry_data_import(
        self,
        retry_data_import_request: "RetryDataImportRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "RetryDataImportResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/RetryDataImport",
            retry_data_import_request,
            RetryDataImportResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )

    async def get_data_import(
        self,
        get_data_import_request: "GetDataImportRequest",
        *,
        timeout: Optional[float] = None,
        deadline: Optional["Deadline"] = None,
        metadata: Optional["MetadataLike"] = None
    ) -> "GetDataImportResponse":
        return await self._unary_unary(
            "/sift.data_imports.v2.DataImportService/GetDataImport",
            get_data_import_request,
            GetDataImportResponse,
            timeout=timeout,
            deadline=deadline,
            metadata=metadata,
        )


class DataImportServiceBase(ServiceBase):

    async def create_data_import_from_url(
        self, create_data_import_from_url_request: "CreateDataImportFromUrlRequest"
    ) -> "CreateDataImportFromUrlResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def create_data_import_from_upload(
        self,
        create_data_import_from_upload_request: "CreateDataImportFromUploadRequest",
    ) -> "CreateDataImportFromUploadResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def detect_config(
        self, detect_config_request: "DetectConfigRequest"
    ) -> "DetectConfigResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def list_data_imports(
        self, list_data_imports_request: "ListDataImportsRequest"
    ) -> "ListDataImportsResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def retry_data_import(
        self, retry_data_import_request: "RetryDataImportRequest"
    ) -> "RetryDataImportResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def get_data_import(
        self, get_data_import_request: "GetDataImportRequest"
    ) -> "GetDataImportResponse":
        raise grpclib.GRPCError(grpclib.const.Status.UNIMPLEMENTED)

    async def __rpc_create_data_import_from_url(
        self,
        stream: "grpclib.server.Stream[CreateDataImportFromUrlRequest, CreateDataImportFromUrlResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.create_data_import_from_url(request)
        await stream.send_message(response)

    async def __rpc_create_data_import_from_upload(
        self,
        stream: "grpclib.server.Stream[CreateDataImportFromUploadRequest, CreateDataImportFromUploadResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.create_data_import_from_upload(request)
        await stream.send_message(response)

    async def __rpc_detect_config(
        self, stream: "grpclib.server.Stream[DetectConfigRequest, DetectConfigResponse]"
    ) -> None:
        request = await stream.recv_message()
        response = await self.detect_config(request)
        await stream.send_message(response)

    async def __rpc_list_data_imports(
        self,
        stream: "grpclib.server.Stream[ListDataImportsRequest, ListDataImportsResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.list_data_imports(request)
        await stream.send_message(response)

    async def __rpc_retry_data_import(
        self,
        stream: "grpclib.server.Stream[RetryDataImportRequest, RetryDataImportResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.retry_data_import(request)
        await stream.send_message(response)

    async def __rpc_get_data_import(
        self,
        stream: "grpclib.server.Stream[GetDataImportRequest, GetDataImportResponse]",
    ) -> None:
        request = await stream.recv_message()
        response = await self.get_data_import(request)
        await stream.send_message(response)

    def __mapping__(self) -> Dict[str, grpclib.const.Handler]:
        return {
            "/sift.data_imports.v2.DataImportService/CreateDataImportFromUrl": grpclib.const.Handler(
                self.__rpc_create_data_import_from_url,
                grpclib.const.Cardinality.UNARY_UNARY,
                CreateDataImportFromUrlRequest,
                CreateDataImportFromUrlResponse,
            ),
            "/sift.data_imports.v2.DataImportService/CreateDataImportFromUpload": grpclib.const.Handler(
                self.__rpc_create_data_import_from_upload,
                grpclib.const.Cardinality.UNARY_UNARY,
                CreateDataImportFromUploadRequest,
                CreateDataImportFromUploadResponse,
            ),
            "/sift.data_imports.v2.DataImportService/DetectConfig": grpclib.const.Handler(
                self.__rpc_detect_config,
                grpclib.const.Cardinality.UNARY_UNARY,
                DetectConfigRequest,
                DetectConfigResponse,
            ),
            "/sift.data_imports.v2.DataImportService/ListDataImports": grpclib.const.Handler(
                self.__rpc_list_data_imports,
                grpclib.const.Cardinality.UNARY_UNARY,
                ListDataImportsRequest,
                ListDataImportsResponse,
            ),
            "/sift.data_imports.v2.DataImportService/RetryDataImport": grpclib.const.Handler(
                self.__rpc_retry_data_import,
                grpclib.const.Cardinality.UNARY_UNARY,
                RetryDataImportRequest,
                RetryDataImportResponse,
            ),
            "/sift.data_imports.v2.DataImportService/GetDataImport": grpclib.const.Handler(
                self.__rpc_get_data_import,
                grpclib.const.Cardinality.UNARY_UNARY,
                GetDataImportRequest,
                GetDataImportResponse,
            ),
        }
